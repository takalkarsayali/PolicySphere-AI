# ğŸŒ PolicySphere AI

## Intelligent Company Policy Assistant

PolicySphere AI is a Retrieval-Augmented Generation (RAG) application that allows users to upload company policy PDFs and ask contextual questions. The system retrieves relevant policy sections using semantic search and generates answers strictly from the provided documents.

---

## ğŸš€ Features

- ğŸ“„ Upload multiple company policy PDFs
- ğŸ§  Semantic search using FAISS vector store
- ğŸ’¬ Conversational memory (context-aware follow-up questions)
- ğŸ”’ Strict prompt guardrails (no hallucinations)
- ğŸ“Œ Source citation (File name + Page number)
- ğŸ—‚ Clean project structure (uploads separated from code)

---

## ğŸ— Architecture Overview

```
User Question
      â†“
Convert to Embedding
      â†“
FAISS Vector Search
      â†“
Retrieve Relevant Chunks
      â†“
LLM (Groq - LLaMA 3.1)
      â†“
Strict Prompt Enforcement
      â†“
Answer + Source Citation
```

---

## ğŸ“ Project Structure

```
PolicySphere-AI/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ data/
â”‚     â””â”€â”€ uploads/        # Uploaded PDFs
â”‚
â”œâ”€â”€ vectorstore/          # (Optional) Stored FAISS index
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ›  Tech Stack

- **Streamlit** â€“ UI
- **LangChain** â€“ RAG framework
- **FAISS** â€“ Vector database
- **HuggingFace Embeddings**
- **Groq (LLaMA 3.1-8B)** â€“ LLM
- **Python**

---

## âš™ Installation

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/your-username/PolicySphere-AI.git
cd PolicySphere-AI
```

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate      # Mac/Linux
venv\Scripts\activate         # Windows
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Setup Environment Variables

Create a `.env` file in root directory:

```
GROQ_API_KEY=your_groq_api_key_here
```

---

## â–¶ Run the Application

```bash
streamlit run main.py
```

---

## ğŸ“Œ How It Works

1. Upload one or more policy PDFs.
2. PDFs are stored in `data/uploads/`.
3. Documents are split into chunks.
4. Chunks are converted into embeddings.
5. Embeddings are stored in FAISS.
6. User question is converted into embedding.
7. Most relevant chunks are retrieved.
8. LLM generates answer strictly from retrieved context.
9. File name and page number are displayed as sources.

---

## ğŸ” Strict Answering Policy

The system is designed to:

- Answer ONLY from uploaded documents
- Avoid assumptions
- Prevent hallucinations
- Return:  
  `"The policy document does not contain this information."`  
  if answer is not found

---

## ğŸ’¡ Example Use Cases

- HR policy assistant
- Employee handbook Q&A
- Compliance document search
- Legal document retrieval
- Internal knowledge assistant

---

## ğŸ§  Future Improvements

- Persist FAISS index to disk
- ChatGPT-style chat UI
- PDF page preview
- Multi-user authentication
- Cloud deployment
- Replace FAISS with Pinecone (for scalability)

---

## ğŸ“œ License

This project is for educational and demonstration purposes.

---

## ğŸ‘©â€ğŸ’» Author

**Sayali Takalkar**  
Software & Data Engineering Enthusiast  
India ğŸ‡®ğŸ‡³

---

â­ If you found this project helpful, consider giving it a star!
